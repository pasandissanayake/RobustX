{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# How to contribute?\n",
    "\n",
    "Here we report a simple guide with the key features of RobustX in order to implement your own (robust) counterfactual explanation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary components\n",
    "from robustx.lib.models.BaseModel import BaseModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from robustx.lib.models.pytorch_models.SimpleNNModel import SimpleNNModel\n",
    "from robustx.datasets.ExampleDatasets import get_example_dataset\n",
    "from robustx.lib.tasks.ClassificationTask import ClassificationTask\n",
    "from robustx.generators.CE_methods.Wachter import Wachter\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from robustx.generators.CEGenerator import CEGenerator\n",
    "\n",
    "\n",
    "class EnsembleModelWrapper(BaseModel):\n",
    "    def __init__(self, model_ensemble: list[SimpleNNModel], aggregation_method: str = 'majority_vote'):\n",
    "        super().__init__(EnsembleModelWrapper)\n",
    "        self.model_ensemble = model_ensemble\n",
    "        self.pt_model_ensemble = [model._model for model in model_ensemble]\n",
    "        self.aggregation_method = aggregation_method\n",
    "    \n",
    "    def train(self, X: pd.DataFrame, y: pd.DataFrame) -> None:\n",
    "        print(\"Training should be done on individual models in the ensemble.\")\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        device = next(self.pt_model_ensemble[0].parameters()).device\n",
    "        X_tensor = torch.Tensor(X.to_numpy()).to(device)\n",
    "        preds_ensemble = []\n",
    "        for model in self.pt_model_ensemble:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(X_tensor).cpu().numpy()\n",
    "                preds_ensemble.append(outputs)\n",
    "        preds_ensemble = np.array(preds_ensemble)  # Shape: (n_models, n_samples, n_classes)\n",
    "        if self.aggregation_method == 'majority_vote':\n",
    "            final_preds = np.round(np.mean(preds_ensemble, axis=0)) # Shape: (n_samples, n_classes)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown aggregation method: {self.aggregation_method}\")\n",
    "        predictions = final_preds.astype(int)\n",
    "        \n",
    "        return pd.DataFrame(predictions, columns=['prediction'], index=X.index)\n",
    "    \n",
    "    def predict_single(self, X: pd.DataFrame) -> int:\n",
    "        return self.predict(X).values.item()\n",
    "    \n",
    "    def predict_ensemble_proba_tensor(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        device = next(self.pt_model_ensemble[0].parameters()).device\n",
    "        X = X.to(device)\n",
    "        probs_ensemble = []\n",
    "        for model in self.pt_model_ensemble:\n",
    "            model.eval()\n",
    "            outputs = model(X)\n",
    "            probs_ensemble.append(outputs)\n",
    "        probs_ensemble = torch.stack(probs_ensemble, dim=0)  # Shape: (n_models, n_samples, n_classes)\n",
    "        return probs_ensemble\n",
    "    \n",
    "    def predict_proba(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X_tensor = torch.Tensor(X.to_numpy())\n",
    "        probs_ensemble = self.predict_ensemble_proba_tensor(X_tensor).numpy()  # Shape: (n_models, n_samples, n_classes)\n",
    "        if self.aggregation_method == 'majority_vote':\n",
    "            aggregated_probs = np.mean(probs_ensemble, axis=0)\n",
    "        return pd.DataFrame(aggregated_probs, columns=[f'class_{i}' for i in range(aggregated_probs.shape[1])], index=X.index)\n",
    "        \n",
    "    def predict_proba_tensor(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        X_numpy = X.numpy()\n",
    "        probabilities = self.predict_proba(X_numpy)\n",
    "        return torch.tensor(probabilities)\n",
    "    \n",
    "    def evaluate(self, X: pd.DataFrame, y: pd.DataFrame):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        report = classification_report(y, y_pred)\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'classification_report': report\n",
    "        }\n",
    "    \n",
    "    def compute_accuracy(self, X_test, y_test):            \n",
    "        return self.evaluate(X_test, y_test)['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T11:40:57.435679Z",
     "start_time": "2025-02-07T11:40:56.501837Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy: 0.4286\n",
      "model accuracy: 0.5714\n",
      "ensemble accuracy: 0.5143\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess dataset\n",
    "dl = get_example_dataset(\"iris\")\n",
    "dl.preprocess(\n",
    "    impute_strategy_numeric='mean',  # Impute missing numeric values with mean\n",
    "    scale_method='minmax',           # Apply min-max scaling\n",
    "    encode_categorical=False         # No categorical encoding needed (since no categorical features)\n",
    ")\n",
    "\n",
    "# remove the target column from the dataset that has labels 2\n",
    "dl.data = dl.data[dl.data['target'] != 2]\n",
    "\n",
    "# Load model, note some RecourseGenerators may only work with a certain type of model,\n",
    "# e.g., MCE only works with a SimpleNNModel\n",
    "n_models = 2\n",
    "model_ensemble = [SimpleNNModel(4, [10], 1, seed=0) for _ in range(n_models)]\n",
    "\n",
    "target_column = \"target\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(dl.data.drop(columns=[target_column]), dl.data[target_column], test_size=0.35, random_state=0)\n",
    "\n",
    "\n",
    "# Train each model in the ensemble\n",
    "all_indexes = np.arange(X_train.shape[0])\n",
    "for model in model_ensemble:\n",
    "    np.random.shuffle(all_indexes)\n",
    "    sampled_indexes = all_indexes[:int(0.8 * len(all_indexes))]\n",
    "    model.train(X_train.iloc[sampled_indexes], y_train.iloc[sampled_indexes], epochs=100, batch_size=16, verbose=0)\n",
    "    print(f\"model accuracy: {model.compute_accuracy(X_test.values, y_test.values):0.4f}\")\n",
    "\n",
    "emodel = EnsembleModelWrapper(model_ensemble=model_ensemble, aggregation_method='majority_vote')\n",
    "print(f\"ensemble accuracy: {emodel.compute_accuracy(X_test, y_test):0.4f}\")\n",
    "\n",
    "\n",
    "# Create task\n",
    "task = ClassificationTask(emodel, dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of an already implemented CE generation method in RobustX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropicRisk(torch.nn.Module):\n",
    "    def __init__(self, theta):\n",
    "        super(EntropicRisk, self).__init__()\n",
    "        self.theta = theta\n",
    "\n",
    "    def forward(self, ensemble_proba):\n",
    "        # ensemble_proba contains prediction probs of each model in ensemble\n",
    "        # for the target class, shape: (n_models, batch_size)\n",
    "        exp_logits = torch.exp(1 - self.theta * ensemble_proba)  # Apply exponential and 1-theta*m(x) loss\n",
    "        avg_exp_logits = torch.mean(exp_logits, dim=0)  # Average over models, Shape: (batch_size)        \n",
    "        risk = (1 / self.theta) * torch.log(avg_exp_logits + 1e-10)  # Add small constant for numerical stability\n",
    "        return risk\n",
    "\n",
    "\n",
    "# Implement the RecourseGenerator class\n",
    "class EntropicCE(CEGenerator):\n",
    "    # You must implement the _generation_method function, this returns the CE for a given\n",
    "    # instance, if you take any extra arguments make sure to specify them before **kwargs,\n",
    "    # like we have done for n and seed (they must have some default value)\n",
    "    def _generation_method(self, \n",
    "                           instance, \n",
    "                           column_name=\"target\", \n",
    "                           neg_value=0,\n",
    "                           max_iter=10,\n",
    "                           theta=1.0,\n",
    "                           tau=0.5,\n",
    "                           lr=0.01,\n",
    "                           epsilon=0.1,\n",
    "                           seed=None, \n",
    "                           ref_model_idx=None,\n",
    "                           device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                           verbose=False,\n",
    "                           **kwargs):\n",
    "        # Remember, the RecourseGenerator has access to its Task! Use this to get access to your dataset or model,\n",
    "        # or to use any of their methods, here we use the ClassificationTask's get_random_positive_instance() method\n",
    "\n",
    "        model_ensemble = self.task.model.model_ensemble\n",
    "        n_models = len(model_ensemble)\n",
    "        if ref_model_idx is None:\n",
    "            ref_model_idx = np.random.randint(0, n_models)\n",
    "        \n",
    "        ref_model = model_ensemble[ref_model_idx]\n",
    "        ref_task = ClassificationTask(ref_model, self.task.training_data)\n",
    "        ce_gen = Wachter(ref_task)\n",
    "        ref_ce = ce_gen.generate_for_instance(instance, neg_value=neg_value, device=device)\n",
    "\n",
    "        ref_ce = torch.Tensor(ref_ce.to_numpy()).to(device)\n",
    "        ent_ce = torch.autograd.Variable(ref_ce.clone(), requires_grad=True).to(device)\n",
    "\n",
    "        optimiser = torch.optim.Adam([ent_ce], lr, amsgrad=True)\n",
    "        entropic_risk = EntropicRisk(theta).to(device)\n",
    "\n",
    "        iterations = 0\n",
    "        cf_is_valid = False\n",
    "        while not cf_is_valid and iterations <= max_iter:\n",
    "            optimiser.zero_grad()\n",
    "            class_prob = self.task.model.predict_ensemble_proba_tensor(ent_ce) # Shape: (n_models, 1)\n",
    "            \n",
    "            risk = entropic_risk(class_prob)  # Assuming binary classification, get probs for positive class\n",
    "            risk.sum().backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            # break conditions\n",
    "            if risk.item() < tau:\n",
    "                cf_is_valid = True\n",
    "            iterations += 1\n",
    "            if verbose:\n",
    "                print(f\"Iteration {iterations:02d}: Entropic risk = {risk.item():.4f}\")\n",
    "                print(f\"Current CE: {ent_ce.detach().cpu().numpy()}\")\n",
    "        if not cf_is_valid:\n",
    "            print(\"Warning: Entropic CE generation did not converge to a valid counterfactual within the max iterations.\")\n",
    "        res = pd.DataFrame(ent_ce.detach().numpy())\n",
    "        res.columns = instance.index\n",
    "        \n",
    "        return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T11:41:02.196196Z",
     "start_time": "2025-02-07T11:40:57.436946Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative instances shape:  (50, 4)\n",
      "Example of a prediction for a negative instance:\n",
      "\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0           0.222222             0.625           0.067797          0.041667\n",
      "Output:  0\n",
      "Class:  0\n",
      "\n",
      "Generating counterfactual explanations using STCE for the first 5 negative instances:\n",
      "Iteration 01: Entropic risk = 0.4869\n",
      "Current CE: [[ 0.48824942  0.3617554   0.32373184 -0.22171657]]\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0           0.488249          0.361755           0.323732         -0.221717\n",
      "Output:  0.5204125046730042\n",
      "Class:  1\n"
     ]
    }
   ],
   "source": [
    "# Each counterfactual explanation generator takes the task on creation, it can also take a custom distance function, but for now we will use the default one.\n",
    "ce_gen = EntropicCE(task)\n",
    "\n",
    "# Get negative instances, the default column_name is always \"target\" but you can set it to the name of your dataset's target variable\n",
    "negs = dl.get_negative_instances(neg_value=0, column_name=\"target\")\n",
    "print(\"Negative instances shape: \", negs.shape)\n",
    "print(f\"Example of a prediction for a negative instance:\\n\")\n",
    "print(negs.head(1))\n",
    "print(\"Output: \", emodel.predict(negs.head(1)).values.item())\n",
    "print(\"Class: \", int(emodel.predict(negs.head(1)).values.item() > 0.5))  # Assuming binary classification with threshold 0.5\n",
    "\n",
    "# You can generate for a set of instances stored in a DataFrame\n",
    "print(\"\\nGenerating counterfactual explanations using STCE for the first 5 negative instances:\")\n",
    "ce = ce_gen.generate_for_instance(negs.iloc[0], verbose=True, device='cpu')\n",
    "print(ce)\n",
    "print(\"Output: \", model.predict(ce).values.item())\n",
    "print(\"Class: \", int(model.predict(ce).values.item() > 0.5))  # Assuming binary classification with threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T11:41:03.173345Z",
     "start_time": "2025-02-07T11:41:02.196825Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All outputs are positive?  True\n"
     ]
    }
   ],
   "source": [
    "# You can also implement a method to generate CEs for all the negative instance in one shot\n",
    "ces = ce_gen.generate_for_all(neg_value=0, column_name=\"target\", device='cpu')\n",
    "print(\"All outputs are positive? \", np.all(model.predict(ces)>0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking your method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have finished implementing your method, you can include it into DefaultBenchmark.py file and test it against other methods supported in the library using this lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------+------------+------------+\n",
      "| Method     |   Execution Time (s) |   Validity |   Distance |\n",
      "+============+======================+============+============+\n",
      "| KDTreeNNCE |             0.133706 |          1 |   0.532503 |\n",
      "+------------+----------------------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from robustx.lib.DefaultBenchmark import default_benchmark\n",
    "methods = [\"KDTreeNNCE\",]\n",
    "evaluations = [\"Validity\", \"Distance\"]\n",
    "default_benchmark(task, methods, evaluations, neg_value=0, column_name=\"target\", delta=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robustx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
